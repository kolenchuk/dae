# Denoising autoencoder 
Ціль проєкту: розробити легку модель для швидкого виправлення граматичних помилок в назвах продуктів в пошукових запитах користувачів на e-commerce сайті

Для розробки моделі була обрана архітектуру на базі трансформерів із такими параметрами:
- Вбудовування розмірністю 256
- 8 голів самоуваги (attention heads)
- 4 шари трансформера
- Розмірність feed-forward шару — 1024

Ключові особливості моделі:
- Механізм самоуваги з урахуванням позиційної інформації (Position-aware attention mechanism)
- Нормалізація по шарах (Layer normalization) для забезпечення стабільності навчання
- Dropout для регуляризації моделі та запобігання перенавчанню
- Ініціалізація ваг, оптимізована для задач виправлення тексту

Ключові особливості функції втрат:
- Label smoothing для пом'якшення жорстких кордонів між класами, що сприяє кращій генералізації.
- Вагування позицій символів з акцентом на кінцеві символи, які часто містять помилки у пошукових запитах.
- Пеналізація довжини для зменшення помилок, пов'язаних із зайвими або пропущеними символами.
- Втрати на основі n-грам (біграми та триграми) для забезпечення коректності послідовностей символів у контексті.

Тренувальний датасет розбитий на 3 частини:
- Легкі помилки:
    - Випадкова заміна одного символу.
    - Видалення одного символу.
    - Вставка зайвого символу.
- Середні помилки:    
    - Перестановка двох сусідніх символів.
    - Подвоєння літер.
    - Усічення закінчень слів.
    - Помилки у типовому закінченні слів.
    - Використання поширених реальних помилок із логів.
- Складні помилки:    
    - Помилки введення свайпом.
    - Комбіновані помилки з кількома типами спотворень.
    - Імітація помилок автокорекції.

Для інтеграції моделі в продукт було реалізовано RESTful API, що дозволяє взаємодіяти з пошуковим механізмом у реальному часі. API приймає вхідний пошуковий запит, передає його через DAE для виправлення локальних помилок, після чого виконує додаткову обробку через BART для уточнення семантики.
- Архітектура API: Реалізована на базі Flask із підтримкою масштабованості через Docker.
- Паралельна обробка: Запити обробляються асинхронно для забезпечення високої швидкодії.
- Фільтрація результатів: Після виправлення запиту модель перевіряє результат за еталонним списком назв телефонів, щоб виключити некоректні виправлення.

Інтеграція з BART відбувається через попередньо треновану модель `facebook/bart-base`, адаптовану для виправлення семантичних помилок у пошукових запитах. BART використовується лише у випадках, коли виправлення DAE не відповідає встановленим критеріям якості.

Проєкт розроблений за допомогою PyTorch


## Структура проєкту:
```
dae/
├── data/               # Dataset files
├── models/             # Saved models
├── reports/            # Validation reports
├── src/                # Source code
├── requirements.txt    # Project dependencies
├── app.py              # Скрипт запуску апі
├── Dockerfile          #
├── docker-compose.yaml #
└── README.md
```
## Інсталяція

1. Клонуйте репозиторій:
```bash
git clone https://github.com/kolenchuk/dae
cd dae
```

2. Створіть віртуальне середовище
```
python -m venv pytorch_env
```
3. Активуйте віртуальне середовище
```
# На Windows:
pytorch_env\Scripts\activate
# На Unix чи MacOS:
source pytorch_env/bin/activate
```
```
pip install -r requirements.txt
```
## Запуск API:

Варіант 1: Використання Docker.
```bash
docker-compose up --build
```

Варіант 2: Запуск без Docker.

Переконайтеся, що всі залежності встановлені: 
```bash
pip install -r requirements.txt
```

Запустіть сервер API вручну: 
```
uvicorn app:app --reload --port 8000
```
   
API буде доступне за адресою: [http://127.0.0.1:8000](http://127.0.0.1:8000/)

Перевірка роботи API: 
Використовуйте інструменти типу Postman або CURL для тестування: 
```bash
curl -X 'GET' 'http://127.0.0.1:8001/search?query=iphnoe%2013%20pro' -H 'accept: application/json'
```



